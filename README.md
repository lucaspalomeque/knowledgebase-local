# ğŸ¤– Knowledgebase Local

> Build a complete RAG (Retrieval-Augmented Generation) system using local LLMs in just one weekend!

[![Made with â¤ï¸](https://img.shields.io/badge/Made%20with-â¤ï¸-red.svg)](https://github.com/yourusername/weekend-rag-local)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A **completely local, private, and free** RAG system that lets you chat with your documents using state-of-the-art LLMs like Hermes and Llama - no API keys required!

## ğŸ¯ What You'll Build

By the end of this weekend project, you'll have:

- ğŸ“š **Document Knowledge Base**: Upload PDFs and build a searchable knowledge base
- ğŸ§  **Local AI Brain**: Hermes 2.5 Mistral or Llama 3.1 running completely offline
- ğŸ’¬ **Chat Interface**: Clean Streamlit web app to interact with your documents
- ğŸ” **Smart Search**: Semantic search powered by ChromaDB vector database
- ğŸ”’ **100% Private**: Your documents never leave your machine

![Demo Screenshot](docs/images/demo-screenshot.png)

## âœ¨ Features

- ğŸš€ **Zero API Costs** - Everything runs locally
- ğŸ”’ **Complete Privacy** - Your data stays on your machine
- ğŸ§  **Multiple LLM Support** - Hermes, Llama, Phi-3, and more
- ğŸ“„ **PDF Processing** - Intelligent document chunking and indexing
- ğŸ’¬ **Chat Interface** - Conversational Q&A with source citations
- âš¡ **Fast Setup** - Get running in under 15 minutes
- ğŸ³ **Docker Ready** - One-command deployment
- ğŸ“Š **Performance Monitoring** - Built-in metrics and debugging

## ğŸš€ Quick Start (5 Minutes)

### Prerequisites

- **Hardware**: 8GB RAM minimum, 6GB VRAM recommended
- **OS**: Linux, macOS, or Windows with WSL2
- **Python**: 3.8 or higher

### 1. Install Ollama

```bash
# Linux/macOS
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# Download from https://ollama.ai/download
```

### 2. Download a Model

```bash
# Recommended: Hermes 2.5 Mistral 7B (~4.8GB)
ollama pull adrienbrault/nous-hermes2-mistral:7b-dpo

# Alternative: Llama 3.1 8B (~5.2GB)
ollama pull llama3.1:8b

# Lightweight: Phi-3.5 Mini (~2.5GB)
ollama pull phi3.5:latest
```

### 3. Clone and Setup

```bash
git clone https://github.com/yourusername/weekend-rag-local.git
cd weekend-rag-local

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/macOS
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt
```

### 4. Run the App

```bash
# Start Ollama server (if not already running)
ollama serve &

# Launch the Streamlit app
streamlit run main.py
```

Open your browser to `http://localhost:8501` and start chatting with your documents! ğŸ‰

## ğŸ“š Supported Models

| Model | Size | VRAM | Performance | Quality | Best For |
|-------|------|------|-------------|---------|----------|
| **Hermes 2.5 Mistral 7B** | 4.8GB | 6GB | â­â­â­â­ | â­â­â­â­â­ | Balanced (Recommended) |
| **Llama 3.1 8B** | 5.2GB | 7GB | â­â­â­ | â­â­â­â­â­ | Best Quality |
| **Phi-3.5 Mini** | 2.5GB | 3GB | â­â­â­â­â­ | â­â­â­ | Low-end Hardware |
| **Code Llama 13B** | 8.5GB | 10GB | â­â­ | â­â­â­â­â­ | Code Documents |

## ğŸ—ï¸ Architecture

```
ğŸ“„ PDF Upload â†’ ğŸ”„ LangChain Processing â†’ ğŸ’¾ ChromaDB Storage
                                    â†“
ğŸ’¬ User Question â†’ ğŸ” Vector Search â†’ ğŸ§  Local LLM â†’ ğŸ“± Response
```

For detailed architecture, see [ARCHITECTURE.md](docs/ARCHITECTURE.md)

## ğŸ“– Usage

### 1. Upload Documents
- Click "Upload Documents" in the sidebar
- Select one or more PDF files
- Wait for processing (automatic chunking and indexing)

### 2. Ask Questions
- Type your question in the chat input
- Get AI-powered answers with source citations
- Follow up with additional questions

### 3. Configure Settings
- Adjust similarity threshold for search precision
- Change max retrieved documents
- Modify LLM temperature for creativity
- Switch between different models

## ğŸ› ï¸ Development

### Project Structure

```
weekend-rag-local/
â”œâ”€â”€ ğŸ“ src/                    # Core application code
â”‚   â”œâ”€â”€ config.py             # Configuration settings
â”‚   â”œâ”€â”€ llm_client.py         # Ollama client wrapper
â”‚   â”œâ”€â”€ rag_chain.py          # Main RAG logic
â”‚   â””â”€â”€ document_processor.py # PDF processing
â”œâ”€â”€ ğŸ“ data/                  # Data storage
â”‚   â”œâ”€â”€ uploads/              # Uploaded documents
â”‚   â””â”€â”€ chromadb/             # Vector database
â”œâ”€â”€ ğŸ“ docs/                  # Documentation
â”‚   â”œâ”€â”€ ROADMAP.md            # Development roadmap
â”‚   â””â”€â”€ ARCHITECTURE.md       # System architecture
â”œâ”€â”€ ğŸ“ tests/                 # Test files
â”œâ”€â”€ main.py                   # Streamlit application
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ Dockerfile               # Container setup
â””â”€â”€ docker-compose.yml       # Multi-service deployment
```

### Running Tests

```bash
# Install test dependencies
pip install pytest pytest-cov

# Run tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

### Development Setup

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Install pre-commit hooks
pre-commit install

# Run linting
black src/ tests/
flake8 src/ tests/
```

## ğŸ³ Docker Deployment

### Single Container

```bash
# Build the image
docker build -t weekend-rag-local .

# Run with GPU support
docker run -d \
  --name rag-local \
  --gpus all \
  -p 8501:8501 \
  -v $(pwd)/data:/app/data \
  weekend-rag-local
```

### Multi-Service with Docker Compose

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

## âš¡ Performance Optimization

### Hardware Recommendations

| Use Case | RAM | VRAM | Storage | Model |
|----------|-----|------|---------|--------|
| **Personal Use** | 8GB | 4GB | 10GB | Phi-3.5 Mini |
| **Professional** | 16GB | 8GB | 20GB | Hermes 2.5 Mistral |
| **Power User** | 32GB | 12GB+ | 50GB | Llama 3.1 8B |

### Optimization Tips

```bash
# CPU-only mode (no GPU)
export CUDA_VISIBLE_DEVICES=""

# Limit Ollama memory usage
export OLLAMA_MAX_LOADED_MODELS=1

# Use quantized models for speed
ollama pull hermes2.5-mistral:7b-q4_0
```

## ğŸ”§ Troubleshooting

### Common Issues

**Ollama not connecting:**
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Restart Ollama
pkill ollama
ollama serve
```

**Out of memory errors:**
```bash
# Switch to smaller model
ollama pull phi3.5:latest

# Update config.py
LLM_MODEL = "phi3.5:latest"
```

**Slow performance:**
```bash
# Check GPU usage
nvidia-smi

# Reduce context length
# In config.py: CHUNK_SIZE = 400
```

**ChromaDB errors:**
```bash
# Reset database
rm -rf data/chromadb/*
# Restart application
```

## ğŸ“ˆ Roadmap

- [x] **Phase 1**: Basic RAG with PDF support
- [x] **Phase 2**: Streamlit UI and chat interface
- [ ] **Phase 3**: Multi-format support (DOCX, TXT, HTML)
- [ ] **Phase 4**: Conversation memory and context
- [ ] **Phase 5**: Advanced retrieval techniques
- [ ] **Phase 6**: REST API and integrations
- [ ] **Phase 7**: Multi-user support and authentication

See [ROADMAP.md](docs/ROADMAP.md) for detailed planning.

## ğŸ¤ Contributing

Contributions are welcome! Please read our [Contributing Guide](CONTRIBUTING.md) for details.

### Ways to Contribute

- ğŸ› **Bug Reports**: Found an issue? Open an issue!
- ğŸ’¡ **Feature Requests**: Have an idea? We'd love to hear it!
- ğŸ“ **Documentation**: Help improve our docs
- ğŸ§ª **Testing**: Add tests or test on different hardware
- ğŸ’» **Code**: Submit pull requests for new features

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai/) for making local LLMs accessible
- [LangChain](https://langchain.com/) for the RAG framework
- [ChromaDB](https://www.trychroma.com/) for vector storage
- [Nous Research](https://nousresearch.com/) for the Hermes models
- [Meta](https://ai.meta.com/) for the Llama models

## ğŸ“Š Stats

![GitHub stars](https://img.shields.io/github/stars/yourusername/weekend-rag-local)
![GitHub forks](https://img.shields.io/github/forks/yourusername/weekend-rag-local)
![GitHub issues](https://img.shields.io/github/issues/yourusername/weekend-rag-local)
![GitHub pull requests](https://img.shields.io/github/issues-pr/yourusername/weekend-rag-local)

---

**Built with â¤ï¸ for the AI community**

*Star â­ this repo if you found it helpful!*