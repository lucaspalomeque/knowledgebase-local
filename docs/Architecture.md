# ðŸ—ï¸ System Architecture - Weekend RAG Local

## ðŸ“‹ Overview

This document describes the architecture of our local RAG (Retrieval-Augmented Generation) system, designed for weekend implementation while maintaining production-quality standards.

## ðŸŽ¯ Design Principles

### Core Principles
- **Local-First**: Everything runs on user's machine, no external dependencies
- **Privacy-Focused**: Documents never leave the local environment
- **Cost-Effective**: Zero ongoing operational costs after initial setup
- **Extensible**: Modular design allows easy feature additions
- **Performance-Conscious**: Optimized for consumer hardware

### Technical Principles
- **Simplicity**: Prefer simple solutions over complex ones
- **Modularity**: Clear separation of concerns
- **Testability**: Each component can be tested independently
- **Configurability**: Key parameters exposed to users
- **Observability**: Built-in logging and monitoring

---

## ðŸ›ï¸ High-Level Architecture

```mermaid
graph TB
    User[ðŸ‘¤ User] --> UI[ðŸ–¥ï¸ Streamlit UI]
    UI --> App[ðŸš€ Application Layer]
    App --> RAG[ðŸ§  RAG Engine]
    App --> Doc[ðŸ“„ Document Processor]
    
    RAG --> LLM[ðŸ¤– Local LLM]
    RAG --> Vector[ðŸ’¾ Vector Store]
    RAG --> Memory[ðŸ§­ Memory Manager]
    
    Doc --> Loader[ðŸ“‹ Document Loader]
    Doc --> Splitter[âœ‚ï¸ Text Splitter]
    Doc --> Embed[ðŸ”¢ Embeddings]
    
    LLM --> Ollama[ðŸ¦™ Ollama Server]
    Vector --> Chroma[ðŸŽ¨ ChromaDB]
    Embed --> ST[ðŸ”¤ SentenceTransformers]
    
    subgraph "Local Environment"
        UI
        App
        RAG
        Doc
        LLM
        Vector
        Memory
        Loader
        Splitter
        Embed
        Ollama
        Chroma
        ST
    end
```

---

## ðŸ”§ Component Architecture

### 1. Presentation Layer

#### Streamlit UI (`main.py`)
```python
Responsibilities:
â”œâ”€â”€ User Interface Rendering
â”œâ”€â”€ File Upload Handling
â”œâ”€â”€ Chat Interface Management
â”œâ”€â”€ Configuration Controls
â”œâ”€â”€ Error Display & Feedback
â””â”€â”€ Real-time Updates

Key Features:
â”œâ”€â”€ Drag-and-drop file upload
â”œâ”€â”€ Real-time chat streaming
â”œâ”€â”€ Source citation display
â”œâ”€â”€ Performance metrics
â”œâ”€â”€ Model switching interface
â””â”€â”€ Settings configuration
```

**Technologies**: Streamlit, HTML/CSS, JavaScript (minimal)

### 2. Application Layer

#### Main Application Controller (`src/app_controller.py`)
```python
Responsibilities:
â”œâ”€â”€ Request Routing
â”œâ”€â”€ Session Management
â”œâ”€â”€ Component Orchestration
â”œâ”€â”€ Error Handling
â”œâ”€â”€ Performance Monitoring
â””â”€â”€ Configuration Management

Design Pattern: Controller/Coordinator
â”œâ”€â”€ Mediates between UI and business logic
â”œâ”€â”€ Manages application state
â”œâ”€â”€ Handles cross-cutting concerns
â””â”€â”€ Provides unified error handling
```

### 3. Business Logic Layer

#### RAG Engine (`src/rag_engine.py`)
```python
class RAGEngine:
    Components:
    â”œâ”€â”€ DocumentProcessor     # PDF â†’ Chunks
    â”œâ”€â”€ VectorStore          # Semantic Search
    â”œâ”€â”€ LLMClient           # Text Generation
    â”œâ”€â”€ MemoryManager       # Conversation Context
    â””â”€â”€ RetrievalStrategy   # Search Optimization
    
    Core Methods:
    â”œâ”€â”€ ingest_document()   # Add new documents
    â”œâ”€â”€ query()            # Answer questions
    â”œâ”€â”€ get_similar()      # Find related content
    â””â”€â”€ clear_memory()     # Reset conversation
```

#### Document Processor (`src/document_processor.py`)
```python
class DocumentProcessor:
    Pipeline:
    â”œâ”€â”€ Document Loading     # PDF â†’ Raw Text
    â”œâ”€â”€ Text Cleaning       # Normalize & Clean
    â”œâ”€â”€ Intelligent Chunking # Semantic Boundaries
    â”œâ”€â”€ Metadata Extraction # Source Information
    â””â”€â”€ Embedding Generation # Text â†’ Vectors
    
    Supported Formats:
    â”œâ”€â”€ PDF (PyPDF2/PyMuPDF)
    â”œâ”€â”€ DOCX (python-docx)
    â”œâ”€â”€ TXT (plain text)
    â””â”€â”€ Future: HTML, MD, etc.
```

#### LLM Client (`src/llm_client.py`)
```python
class LocalLLMClient:
    Capabilities:
    â”œâ”€â”€ Model Management     # Load/Switch models
    â”œâ”€â”€ Generation          # Text completion
    â”œâ”€â”€ Streaming          # Real-time responses
    â”œâ”€â”€ Error Recovery     # Fallback strategies
    â””â”€â”€ Performance Monitor # Token/sec tracking
    
    Supported Models:
    â”œâ”€â”€ Hermes 2.5 Mistral 7B (Primary)
    â”œâ”€â”€ Llama 3.1 8B (Alternative)
    â”œâ”€â”€ Phi-3.5 Mini (Lightweight)
    â””â”€â”€ Code Llama 13B (Specialized)
```

### 4. Data Layer

#### Vector Store (`src/vector_store.py`)
```python
class VectorStore:
    Architecture:
    â”œâ”€â”€ ChromaDB Client     # Vector Database
    â”œâ”€â”€ Collection Manager  # Index Management
    â”œâ”€â”€ Embedding Function # Text â†’ Vectors
    â”œâ”€â”€ Search Engine      # Similarity Search
    â””â”€â”€ Persistence Layer  # Disk Storage
    
    Operations:
    â”œâ”€â”€ add_documents()    # Store new vectors
    â”œâ”€â”€ similarity_search() # Find similar docs
    â”œâ”€â”€ filter_search()    # Metadata filtering
    â””â”€â”€ get_collection_stats() # Monitoring
```

#### Memory Manager (`src/memory_manager.py`)
```python
class MemoryManager:
    Types:
    â”œâ”€â”€ Conversation Memory # Chat history
    â”œâ”€â”€ Document Context   # Active documents
    â”œâ”€â”€ User Preferences   # Settings/configs
    â””â”€â”€ Session State     # Temporary data
    
    Storage:
    â”œâ”€â”€ In-Memory (Runtime)
    â”œâ”€â”€ Local Files (Persistence)
    â””â”€â”€ Vector Store (Semantic memory)
```

---

## ðŸ”„ Data Flow Architecture

### Document Ingestion Flow
```mermaid
sequenceDiagram
    participant U as User
    participant UI as Streamlit UI
    participant DP as Document Processor
    participant VS as Vector Store
    participant DB as ChromaDB
    
    U->>UI: Upload PDF
    UI->>DP: process_document(file)
    DP->>DP: extract_text()
    DP->>DP: create_chunks()
    DP->>DP: generate_embeddings()
    DP->>VS: store_vectors()
    VS->>DB: persist_to_disk()
    DB-->>VS: confirmation
    VS-->>DP: success
    DP-->>UI: processing_result
    UI-->>U: Upload Complete
```

### Query Processing Flow
```mermaid
sequenceDiagram
    participant U as User
    participant UI as Streamlit UI
    participant RE as RAG Engine
    participant VS as Vector Store
    participant LLM as LLM Client
    participant OL as Ollama
    
    U->>UI: Ask Question
    UI->>RE: query(question)
    RE->>VS: similarity_search(question)
    VS-->>RE: relevant_documents[]
    RE->>RE: build_context()
    RE->>LLM: generate(prompt, context)
    LLM->>OL: HTTP Request
    OL-->>LLM: Response Stream
    LLM-->>RE: Generated Answer
    RE-->>UI: Answer + Sources
    UI-->>U: Display Response
```

---

## ðŸ’¾ Data Architecture

### File System Structure
```
data/
â”œâ”€â”€ uploads/                 # Original uploaded files
â”‚   â”œâ”€â”€ document1.pdf
â”‚   â”œâ”€â”€ document2.pdf
â”‚   â””â”€â”€ ...
â”œâ”€â”€ chromadb/               # Vector database storage
â”‚   â”œâ”€â”€ chroma.sqlite3      # Metadata database
â”‚   â”œâ”€â”€ collections/        # Vector collections
â”‚   â””â”€â”€ embeddings/         # Embedding cache
â”œâ”€â”€ cache/                  # Application cache
â”‚   â”œâ”€â”€ embeddings/         # Embedding cache
â”‚   â”œâ”€â”€ models/            # Model cache
â”‚   â””â”€â”€ temp/              # Temporary files
â””â”€â”€ logs/                   # Application logs
    â”œâ”€â”€ app.log
    â”œâ”€â”€ performance.log
    â””â”€â”€ error.log
```

### Vector Database Schema
```python
# ChromaDB Collection Structure
Collection: "weekend_rag"
â”œâ”€â”€ Documents: List[str]     # Chunk content
â”œâ”€â”€ Embeddings: List[float]  # Vector embeddings
â”œâ”€â”€ Metadata: Dict[str, Any] # Source information
â”‚   â”œâ”€â”€ source_file: str     # Original filename
â”‚   â”œâ”€â”€ chunk_id: str        # Unique identifier
â”‚   â”œâ”€â”€ chunk_index: int     # Position in document
â”‚   â”œâ”€â”€ file_hash: str       # File checksum
â”‚   â”œâ”€â”€ timestamp: datetime  # Processing time
â”‚   â””â”€â”€ content_type: str    # Document type
â””â”€â”€ IDs: List[str]          # Unique identifiers
```

### Memory Management
```python
# Session State Structure (Streamlit)
st.session_state = {
    "rag_engine": RAGEngine,           # Core engine instance
    "messages": List[Dict],            # Chat history
    "uploaded_files": List[str],       # Processed files
    "current_model": str,              # Active LLM model
    "config": Dict[str, Any],          # User settings
    "performance_metrics": Dict        # Runtime metrics
}
```

---

## âš¡ Performance Architecture

### Memory Management Strategy
```python
# Memory allocation priorities
High Priority (Always in RAM):
â”œâ”€â”€ Active LLM model        # ~5-8GB
â”œâ”€â”€ Embedding model         # ~500MB
â”œâ”€â”€ ChromaDB client         # ~200MB
â””â”€â”€ Current conversation    # ~10MB

Medium Priority (Cache):
â”œâ”€â”€ Recent embeddings       # ~1GB
â”œâ”€â”€ Frequent documents      # ~500MB
â””â”€â”€ Model artifacts         # ~2GB

Low Priority (Disk):
â”œâ”€â”€ Full document store     # Unlimited
â”œâ”€â”€ Historical conversations # Unlimited
â””â”€â”€ Performance logs        # Unlimited
```

### Caching Strategy
```python
# Multi-level caching approach
Level 1 - Memory Cache (LRU):
â”œâ”€â”€ Recent embeddings       # 1000 items
â”œâ”€â”€ Frequent queries        # 500 items
â””â”€â”€ Model outputs          # 100 items

Level 2 - Disk Cache:
â”œâ”€â”€ Embedding cache         # Persistent
â”œâ”€â”€ Model downloads         # Persistent
â””â”€â”€ Processed documents     # Persistent

Level 3 - Vector Database:
â”œâ”€â”€ All document embeddings # Persistent
â”œâ”€â”€ Metadata index         # Persistent
â””â”€â”€ Collection statistics  # Persistent
```

### Concurrency Model
```python
# Threading strategy
Main Thread:
â”œâ”€â”€ Streamlit UI rendering
â”œâ”€â”€ User interaction handling
â””â”€â”€ Session state management

Background Threads:
â”œâ”€â”€ Document processing     # CPU intensive
â”œâ”€â”€ Embedding generation    # GPU/CPU intensive
â”œâ”€â”€ Vector storage         # I/O intensive
â””â”€â”€ Model inference        # GPU intensive

Thread Pool Configuration:
â”œâ”€â”€ Max workers: 4 (CPU cores)
â”œâ”€â”€ Queue size: 10 (pending tasks)
â””â”€â”€ Timeout: 300s (long operations)
```

---

## ðŸ”’ Security Architecture

### Data Protection
```python
# Security principles
Local-Only Processing:
â”œâ”€â”€ No external API calls
â”œâ”€â”€ No data transmission
â”œâ”€â”€ No cloud dependencies
â””â”€â”€ Complete user control

Input Validation:
â”œâ”€â”€ File type validation
â”œâ”€â”€ File size limits
â”œâ”€â”€ Content sanitization
â””â”€â”€ Path traversal prevention

Resource Protection:
â”œâ”€â”€ Memory usage limits
â”œâ”€â”€ Disk space monitoring
â”œâ”€â”€ CPU usage throttling
â””â”€â”€ GPU memory management
```

### Access Control
```python
# Current implementation (single-user)
File System Access:
â”œâ”€â”€ Read: data/ directory only
â”œâ”€â”€ Write: data/ directory only
â”œâ”€â”€ Execute: Python scripts only
â””â”€â”€ Network: localhost only

# Future multi-user considerations
User Isolation:
â”œâ”€â”€ Per-user data directories
â”œâ”€â”€ Session-based access control
â”œâ”€â”€ Document sharing permissions
â””â”€â”€ API authentication
```

---

## ðŸ“Š Monitoring Architecture

### Performance Metrics
```python
# Real-time monitoring
System Metrics:
â”œâ”€â”€ CPU usage percentage
â”œâ”€â”€ Memory consumption (RAM/VRAM)
â”œâ”€â”€ Disk I/O operations
â””â”€â”€ Network connections

Application Metrics:
â”œâ”€â”€ Response times (p50, p95, p99)
â”œâ”€â”€ Query throughput (queries/minute)
â”œâ”€â”€ Error rates (by component)
â””â”€â”€ Model performance (tokens/second)

Business Metrics:
â”œâ”€â”€ Documents processed
â”œâ”€â”€ Questions answered
â”œâ”€â”€ User satisfaction (implicit)
â””â”€â”€ Feature usage statistics
```

### Logging Strategy
```python
# Structured logging approach
Log Levels:
â”œâ”€â”€ DEBUG: Detailed debugging info
â”œâ”€â”€ INFO: General application flow
â”œâ”€â”€ WARNING: Potential issues
â”œâ”€â”€ ERROR: Handled exceptions
â””â”€â”€ CRITICAL: System failures

Log Destinations:
â”œâ”€â”€ Console: Development
â”œâ”€â”€ Files: Production
â”œâ”€â”€ Structured: JSON format
â””â”€â”€ Rotation: Size/time based

Key Log Events:
â”œâ”€â”€ Document processing
â”œâ”€â”€ Query execution
â”œâ”€â”€ Model loading/switching
â”œâ”€â”€ Error conditions
â””â”€â”€ Performance metrics
```

---

## ðŸš€ Deployment Architecture

### Local Development
```bash
# Development stack
Application Server:
â”œâ”€â”€ Streamlit dev server    # Port 8501
â”œâ”€â”€ Hot reload enabled      # Code changes
â”œâ”€â”€ Debug mode active       # Detailed logs
â””â”€â”€ Local file watching     # Auto-restart

Dependencies:
â”œâ”€â”€ Ollama server          # Port 11434
â”œâ”€â”€ Python virtual env     # Isolated packages
â”œâ”€â”€ Local ChromaDB         # File-based storage
â””â”€â”€ Development tools      # Testing, linting
```

### Production Deployment
```yaml
# Docker composition
services:
  app:
    build: .
    ports: ["8501:8501"]
    volumes: ["./data:/app/data"]
    environment:
      - OLLAMA_URL=http://ollama:11434
    depends_on: [ollama]
  
  ollama:
    image: ollama/ollama:latest
    ports: ["11434:11434"]
    volumes: ["ollama_data:/root/.ollama"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

### Cloud Deployment Options
```python
# Platform considerations
AWS EC2:
â”œâ”€â”€ GPU instances (g4dn.xlarge)
â”œâ”€â”€ EBS storage for data
â”œâ”€â”€ Security groups for access
â””â”€â”€ Auto-scaling (future)

Google Cloud:
â”œâ”€â”€ Compute Engine with GPUs
â”œâ”€â”€ Persistent disks
â”œâ”€â”€ VPC networking
â””â”€â”€ Cloud Run (containerized)

Self-Hosted:
â”œâ”€â”€ Docker Compose setup
â”œâ”€â”€ Reverse proxy (nginx)
â”œâ”€â”€ SSL termination
â””â”€â”€ Backup strategies
```

---

## ðŸ”„ Extension Points

### Plugin Architecture (Future)
```python
# Extensibility design
Document Loaders:
â”œâ”€â”€ Base: DocumentLoader
â”œâ”€â”€ PDF: PyPDFLoader
â”œâ”€â”€ DOCX: DocxLoader
â”œâ”€â”€ Custom: UserDefinedLoader
â””â”€â”€ Future: WebLoader, APILoader

LLM Providers:
â”œâ”€â”€ Base: BaseLLMClient
â”œâ”€â”€ Ollama: OllamaClient
â”œâ”€â”€ Local: TransformersClient
â”œâ”€â”€ Custom: CustomLLMClient
â””â”€â”€ Future: CloudLLMClient

Vector Stores:
â”œâ”€â”€ Base: BaseVectorStore
â”œâ”€â”€ ChromaDB: ChromaVectorStore
â”œâ”€â”€ FAISS: FAISSVectorStore
â”œâ”€â”€ Custom: CustomVectorStore
â””â”€â”€ Future: PineconeVectorStore
```

### API Extension Points
```python
# Future API endpoints
REST API Routes:
â”œâ”€â”€ POST /documents        # Upload documents
â”œâ”€â”€ GET /documents         # List documents
â”œâ”€â”€ DELETE /documents/{id} # Remove document
â”œâ”€â”€ POST /query           # Ask questions
â”œâ”€â”€ GET /health           # System status
â””â”€â”€ WebSocket /chat       # Real-time chat

Integration Hooks:
â”œâ”€â”€ Pre-processing hooks   # Document modification
â”œâ”€â”€ Post-processing hooks  # Response enhancement
â”œâ”€â”€ Authentication hooks   # User validation
â””â”€â”€ Monitoring hooks      # Custom metrics
```

---

## ðŸ“ˆ Scaling Considerations

### Horizontal Scaling (Future)
```python
# Multi-instance architecture
Load Balancer:
â”œâ”€â”€ Request distribution
â”œâ”€â”€ Health checking
â”œâ”€â”€ Session affinity
â””â”€â”€ SSL termination

Application Instances:
â”œâ”€â”€ Stateless design
â”œâ”€â”€ Shared data layer
â”œâ”€â”€ Distributed caching
â””â”€â”€ Async processing

Data Layer:
â”œâ”€â”€ Distributed vector DB
â”œâ”€â”€ Shared file storage
â”œâ”€â”€ Centralized logging
â””â”€â”€ Metrics aggregation
```

### Performance Optimization
```python
# Optimization strategies
Model Optimization:
â”œâ”€â”€ Quantization (4-bit, 8-bit)
â”œâ”€â”€ Model pruning
â”œâ”€â”€ Knowledge distillation
â””â”€â”€ Hardware-specific optimization

Inference Optimization:
â”œâ”€â”€ Batch processing
â”œâ”€â”€ Caching strategies
â”œâ”€â”€ Parallel processing
â””â”€â”€ GPU memory optimization

Data Optimization:
â”œâ”€â”€ Embedding compression
â”œâ”€â”€ Index optimization
â”œâ”€â”€ Query caching
â””â”€â”€ Prefetching strategies
```

---

## ðŸŽ¯ Design Decisions & Trade-offs

### Key Decisions

| Decision | Rationale | Trade-offs |
|----------|-----------|------------|
| **Ollama over vLLM** | Easier setup, better UX | Lower peak performance |
| **ChromaDB over FAISS** | Built-in persistence, easier API | Higher memory usage |
| **Streamlit over FastAPI+React** | Faster development, integrated | Less customization |
| **Local-only architecture** | Privacy, cost, simplicity | Scalability limitations |
| **Python over Rust/Go** | ML ecosystem, rapid development | Runtime performance |

### Performance Trade-offs

| Aspect | Choice | Pro | Con |
|--------|-------|-----|-----|
| **Model Size** | 7B parameters | Good quality, reasonable speed | High memory usage |
| **Chunk Size** | 800 tokens | Good context, fast search | May split concepts |
| **Vector Dimensions** | 384 (MiniLM) | Fast search, low memory | Reduced semantic resolution |
| **Context Window** | 4K tokens | Good context retention | Higher inference cost |

### Future Considerations

```python
# Architectural evolution path
Phase 1 (Current):
â”œâ”€â”€ Single-user, local-only
â”œâ”€â”€ Basic RAG pipeline
â”œâ”€â”€ Simple UI
â””â”€â”€ Core functionality

Phase 2 (Next weekend):
â”œâ”€â”€ Multi-format support
â”œâ”€â”€ Advanced retrieval
â”œâ”€â”€ Conversation memory
â””â”€â”€ Performance optimization

Phase 3 (Future):
â”œâ”€â”€ Multi-user support
â”œâ”€â”€ Cloud deployment
â”œâ”€â”€ API development
â””â”€â”€ Plugin architecture

Phase 4 (Advanced):
â”œâ”€â”€ Multi-modal capabilities
â”œâ”€â”€ Agent functionality
â”œâ”€â”€ Enterprise features
â””â”€â”€ AI model fine-tuning
```

---

## ðŸ“š References & Further Reading

### Technical Documentation
- [LangChain Documentation](https://python.langchain.com/)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [Ollama Documentation](https://ollama.ai/docs)
- [Streamlit Documentation](https://docs.streamlit.io/)

### Research Papers
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
- [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906)
- [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)

### Model Documentation
- [Hermes 2.5 Model Card](https://huggingface.co/NousResearch/Hermes-2.5-Mistral-7B)
- [Llama 3.1 Model Card](https://github.com/facebookresearch/llama)
- [Phi-3.5 Technical Report](https://arxiv.org/abs/2404.14219)

---

**Document Version**: 1.0  
**Last Updated**: Project Start Date  
**Maintainer**: Your Name  
**Review Cycle**: After each development phase